---
title: "Compilation Report | Historische Entscheidungen des Bundesgerichtshofs in Strafsachen"
author: Seán Fobbe und Tilko Swalve
geometry: margin=3cm
fontsize: 11pt
papersize: a4
output:
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: true
    pandoc_args: --listings
    includes:
      in_header: tex/Preamble_DE.tex
      before_body: [temp/Definitions.tex, tex/Titlepage_Compilation.tex]
bibliography: temp/packages.bib
nocite: '@*'
---



```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = TRUE,
                      message = TRUE,
                      collapse = TRUE,
                      comment = "#>")
```




```{r, results = "asis", echo = FALSE}
cat(readLines("README.md"),
    sep = "\n")
```



# Packages laden


```{r}

library(targets)
library(tarchetypes)
library(RcppTOML)
library(future)
library(data.table)
library(quanteda)
library(knitr)
library(kableExtra)
library(igraph)
library(ggraph)

tar_unscript()
```



# Vorbereitung

## Definitionen

```{r}

## Datum
datestamp <- Sys.Date()
print(datestamp)

## Datum und Uhrzeit (Beginn)
begin.script <- Sys.time()

## Konfiguration
config <- RcppTOML::parseTOML("config.toml")
print(config)


# Analyse-Ordner
dir.analysis <- paste0(getwd(),
                       "/analysis")


```


## Aufräumen

Löscht Dateien im Output-Ordner, die nicht vom heutigen Tag sind.


```{r}

unlink(list.files(pattern = "\\.tiff"))

unlink(grep(datestamp,
            list.files("output",
                       full.names = TRUE),
            invert = TRUE,
            value = TRUE))

unlink("temp")
##unlink("output", recursive = TRUE)


if(config$debug$tesseractresume == FALSE){
    
    unlink("temp_tesseract")
    
    }


```



## Ordner erstellen

```{r}


dirs <- c("output",
          "temp",
          "files")

lapply(dirs, dir.create, showWarnings = FALSE, recursive = TRUE)

dir.create(dir.analysis, showWarnings = FALSE)

```




## Vollzitate statistischer Software schreiben

```{r}
knitr::write_bib(renv::dependencies()$Package,
                 "temp/packages.bib")
```




# Globale Variablen


## Packages definieren

```{targets global-packages, tar_globals = TRUE}

tar_option_set(packages = c("tarchetypes",  # Zusätzliche Targets Funktionen
                            "RcppTOML",     # TOML-Dateien lesen und schreiben
                            "fs",           # Verbessertes File Handling
                            "zip",          # Verbessertes ZIP Handling
                            "testthat",     # Unit tests
                            "mgsub",        # Vektorisiertes Gsub
                            "httr",         # HTTP-Werkzeuge
                            "rvest",        # HTML/XML-Extraktion
                            "knitr",        # Professionelles Reporting
                            "kableExtra",   # Verbesserte Kable Tabellen
                            "pdftools",     # Verarbeitung von PDF-Dateien
                            "ggplot2",      # Fortgeschrittene Datenvisualisierung
							"ggraph",       # Visualisierung von Graphen
                            "scales",       # Skalierung von Diagrammen
                            "magick",       # Image processing
                            "readtext",     # TXT-Dateien einlesen
                            "quanteda",     # Fortgeschrittene Computerlinguistik
                            "future",       # Parallelisierung
                            "future.apply", # Funktionen höherer Ordnung für Parallelisierung
                            "data.table"))   # Fortgeschrittene Datenverarbeitung

tar_option_set(workspace_on_error = TRUE) # Save Workspace on Error
tar_option_set(format = "qs")

```


## Konfiguration


```{targets global-config, tar_globals = TRUE}

datestamp <- Sys.Date()

config <- RcppTOML::parseTOML("config.toml")

dir.analysis <- paste0(getwd(),
                       "/analysis")

## Caption for diagrams
caption <- paste("Fobbe/Swalve | DOI:",
                 config$doi$data$version)


## Prefix for figure titles
prefix.figuretitle <- paste(config$project$shortname,
                            "| Version",
                            config$version$semantic)

## File prefix
prefix.files <- paste0(config$project$shortname,
                       "_",
                       config$version$dash)


if (config$cores$max == TRUE){
    fullCores <- parallel::detectCores()
	tessJobs <- round(fullCores / 3)
}


if (config$cores$max == FALSE){
    fullCores <- as.integer(config$cores$number)
    tessJobs <- as.integer(config$cores$tessjobs)
}

```




## Funktionen definieren

```{targets global-functions, tar_globals = TRUE}

lapply(list.files("functions", pattern = "\\.R$", full.names = TRUE), source)


```



## Metadaten für TXT-Dateien definieren

```{targets global-txtvars, tar_globals = TRUE}

docvarnames <- c("spruchkoerper_az",
                 "registerzeichen",
                 "eingangsnummer",
                 "eingangsjahr_az",
                 "zusatz_az",
                 "name",
                 "kollision")

```


## ZIP-Datei für Source definieren

```{targets global-sourcefiles, tar_globals = TRUE}

files.source.raw <-  c(system2("git", "ls-files", stdout = TRUE), ".git")

```






# Pipeline: Konstruktion




## File Tracking Targets

Mit diesem Abschnitt der Pipeline werden Input-Dateien getrackt. Mit der Option \enquote{format = "file"} werden für Input-Dateien Prüfsummen berechnet. Falls sich diese verändern werden alle von ihnen abhängigen Pipeline-Schritte als veraltet markiert und neu berechnet.



### Source Code

Dies sind alle Dateien, die den Source Code für den Datensatz bilden.

```{targets tar.file.source}
tar_target(files.source,
           files.source.raw,
           format = "file")

```


### Changelog

```{targets tar.file.changelog}
tar_target(changelog,
           "CHANGELOG.md",
           format = "file")
```




### Datenmodell

Dieses Target liest das maschinenlesbare Datenmodell für den Datensatz ein.

```{targets tar.file.datamodel}
list(
    tar_target(file.datamodel,
               "data/BGH-Strafrecht_Datamodel.csv",
               format = "file"),
    tar_target(datamodel,
               fread(file.datamodel))
)
```

### Problematische OCR-Dateien

Diese Dateien verursachen seltsames Verhalten in der OCR-Pipeline. In der Regel reichen 10-15 GB Arbeitsspeicher aus um 5-6 Tesseract Jobs parallel laufen zu lassen, diese Dateien verbrauchen über 64GB Arbeitsspeicher selbst bei nur einem Tesseract Job.


```{targets tar.file.ocrprob}
list(
    tar_target(file.ocrprob,
               "data/ocr-problem-cases.txt",
               format = "file"),
    tar_target(pdf.ocrprob,
               readLines(file.ocrprob))
)
```



### Tabelle mit OCR-Korrekturen

Diese Tabelle enthält Korrekturen für OCR-Fehler.


```{targets tar.file.replacements}
list(
    tar_target(file.replacements,
           "data/BGH-Strafrecht_ReplacementTable.csv",
           format = "file"),
     tar_target(replacements,
                fread(file.replacements))
     )
```




## Vorbereitung des Korpus

### Entpacken und Standardisieren von Dateinamen

```{targets tar.unzip.rename}

tar_target(pdf.original,
           f.unzip_rename(dir.in = "zip_original",
                          dir.out = "files/pdf_original"),
           format = "file")

```

### Testen auf Querformat

```{targets tar.landscape}

tar_target(dt.landscape,
           f.landscape(x = pdf.original))

```

### Querformat zu Hochformat rotieren

```{targets tar.rotate}

tar_target(pdf.rotated,
           f.rotate(dt.landscape$x[dt.landscape$landscape],
                    files.opposite = c("2_StR_481_84_NA_NA_NA.pdf",
                                       "4_StR_131_60_NA_NA_NA.pdf",
                                       "4_StR_190_70_NA_NA_NA.pdf",
                                       "4_StR_512_89_NA_NA_NA.pdf"),
                    angle = -90,
                    dir.output = "files/pdf_rotated",
                    clean = TRUE))

```

### Originale und rotierte PDF-Dateien vereinigen


```{targets tar.original.rotate.unite}

tar_target(pdf.cleaned,
           c(pdf.original[basename(pdf.original) %notin% basename(pdf.rotated)],
             pdf.rotated))

```


## Konvertieren

### Problematische PDF-Dateien entfernen

```{targets tar.convert.removeprob}

tar_target(pdf.cleaned.noprob,
           pdf.cleaned[basename(pdf.cleaned) %notin% basename(pdf.ocrprob)])

```




### Optical Character Recognition (OCR)


```{targets tar.convert.tesseract}

list(tar_target(txt.ocr,
                f.tar_pdf_ocr(pdf.cleaned.noprob,
                              dpi = 300,
                              lang = "deu",
                              output = "txt",
                              resume = config$debug$tesseractresume,
                              crop.firstpage = 0,
                              crop.lastpage = 0,
                              dir.out.pdf = "files/pdf_tesseract",
                              dir.out.txt = "files/txt_tesseract",
                              tempfile = TRUE,
                              chunksperworker = 1,
                              chunksize = 1,
                              quiet = TRUE,
                              jobs = tessJobs),
                format = "file"),
     tar_target(dt.ocr,
                f.readtext(x = txt.ocr,
                           docvarnames = docvarnames))
     )

```


### PDF Text Layer extrahieren

```{targets tar.convert.pdfextract}

list(tar_target(txt.extracted,
                f.tar_pdf_extract(x = pdf.original,
                                  outputdir = "files/txt_extracted",
								  multicore = config$parallel$extractPDF,
                                  cores = fullCores),
                format = "file"),
     tar_target(dt.extracted,
                f.readtext(x = txt.extracted,
                           docvarnames = docvarnames))
     )
					
```					


## Enhance


```{targets tar.enhance.cleantext}

tar_target(var_text,
           f.clean_text(dt.ocr$text))

```


### Variablen erstellen: \enquote{bghst, bghz, bghr, nachschlagewerk}

```{targets tar.enhance.sammlungen}

tar_target(var_sammlungen,
           f.var_sammlungen(var_text))

```



### Variablen erstellen: \enquote{zeichen, token, typen, saetze}

Berechnung klassischer linguistischer Kennzahlen.



```{targets tar.enhance.lingstats}
tar_target(var_lingstats,
                f.lingstats(dt.ocr,
                            multicore = config$parallel$lingsummarize,
                            cores = fullCores,
                            germanvars = TRUE))
```





### Konstanten erstellen

Konstanten die dem Datensatz wichtige Herkunftsinformationen hinzufügen. Darunter sind die Versionsnummer, die Version DOI, die Concept DOI und die Lizenz.



```{targets tar.enhance.constants}
tar_target(var_constants,
           data.frame(version = config$version$semantic,
                      doi_concept = config$doi$data$concept,
                      doi_version = config$doi$data$version,
                      lizenz = as.character(config$license$data))[rep(1,
                                                                      nrow(dt.ocr)),])
```





## Report Targets

Dieser Abschnitt der Pipeline erstellt die finalen Berichte (Codebook und Robustness Checks).





### LaTeX-Definitionen schreiben

Um gewisse Variablen aus der Pipeline in die LaTeX-Kompilierung einzuführen müssen diese als .tex-Datei auf die Festplatte geschrieben werden.

```{targets tar.report.latexdefs}
tar_target(latexdefs,
                f.latexdefs(config,
                            dir = "temp",
                            version = datestamp),
	       format = "file")

```







# Pipeline: Kompilierung



## Durchführen der Kompilierung

```{r pipeline-run, results = "hide"}
tar_make()
```


## Pipeline archivieren


```{r pipeline-zip}
zip(paste0("output/",
           paste0(config$project$shortname,
                  "_",
                  config$version$dash),
           "_Targets_Storage.zip"),
    "_targets/")
```





## Visualisierung

```{r, pipeline-graph, fig.width = 12, fig.height = 18}

edgelist <- tar_network(targets_only = TRUE)$edges
setDT(edgelist)

g  <- igraph::graph_from_data_frame(edgelist,
                                    directed = TRUE)


ggraph(g,
       'sugiyama') + 
    geom_edge_diagonal(colour = "grey")+
    geom_node_point()+
    geom_node_text(aes(label = name),
                   size = 2,
                   repel = TRUE)+
    theme_void()


```



# Liste aller Targets (alphabetisch)

Die vollständige Liste aller Targets, inklusive ihres Types und ihrer Größe. Targets die auf Dateien verweisen (z.B. alle PDF-Dateien) geben die Gesamtgröße der Dateien auf der Festplatte an.



```{r, pipeline-list}

meta <- tar_meta(fields = c("type", "bytes", "format"), complete_only = TRUE)
setDT(meta)
meta$MB <- round(meta$bytes / 1e6, digits = 2)

# Gesamter Speicherplatzverbrauch
sum(meta$MB, na.rm = TRUE)

kable(meta[order(type, name)],
      format = "latex",
      align = "r",
      booktabs = TRUE,
      longtable = TRUE) %>% kable_styling(latex_options = "repeat_header")


```




# Gesamte Laufzeit

```{r, pipeline-runtime}
meta <- tar_meta(fields = c("time", "seconds"), complete_only = TRUE)
setDT(meta)
meta$mins <- round(meta$seconds / 60, digits = 2)

runtime.sum <- sum(meta$seconds)

## Sekunden
print(runtime.sum)

## Minuten
runtime.sum / 60

## Stunden
runtime.sum / 3600
```




# Laufzeit einzelner Targets

Der Zeitpunkt an dem die Targets berechnet wurden und ihre jeweilige Laufzeit in Sekunden.


```{r, pipeline-timing}
kable(meta[order(-seconds)],
      format = "latex",
      align = "r",
      booktabs = TRUE,
      longtable = TRUE) %>% kable_styling(latex_options = "repeat_header")


```


\newpage
# Warnungen



```{r, pipline-warnings, results = 'asis'}

meta <- tar_meta(fields = "warnings", complete_only = TRUE)
setDT(meta)
meta$warnings <- gsub("(\\.pdf|\\.html?|\\.txt)", "\\1 \n\n", meta$warnings)
meta <- meta[name != "dt.annotated"] # de_core_news_sm does not work correctly with lemmatization; warnings are not reported because they flood the document; should be inspected directly

if (meta[,.N > 0]){

    for(i in 1:meta[,.N]){

        cat(paste("##", meta[i]$name), "\n\n")
        cat(paste(meta[i]$warnings, "\n\n"))
        
    }

}else{

    cat("No warnings to report.")

}

```



\newpage
# Fehlermeldungen

```{r, pipeline-errors}

meta <- tar_meta(fields = "error", complete_only = TRUE)
setDT(meta)

if (meta[,.N > 0]){

    for(i in 1:meta[,.N]){

        cat(paste("##", meta[i]$name), "\n\n")
        cat(paste(meta[i]$error, "\n\n"))
        
    }

}else{

    cat("No errors to report.")

}


```







# Dateigrößen




## ZIP-Dateien

```{r filesize.zip}

files <- list.files("output", pattern = "\\.zip", full.names = TRUE)

filesize <- round(file.size(files) / 10^6, digits = 2)

table.size <- data.table(basename(files),
                         filesize)


kable(table.size,
      format = "latex",
      align = c("l", "r"),
      format.args = list(big.mark = ","),
      booktabs = TRUE,
      longtable = TRUE,
      col.names = c("Datei",
                    "Größe in MB"))

```





\newpage

```{r, results = "asis", echo = FALSE}
cat(readLines("CHANGELOG.md"),
    sep = "\n")

```


# Abschluss

```{r}

## Datumsstempel
print(datestamp) 

## Datum und Uhrzeit (Anfang)
print(begin.script)


## Datum und Uhrzeit (Ende)
end.script <- Sys.time()
print(end.script)


## Laufzeit des gesamten Skriptes
print(end.script - begin.script)

```


# Parameter für strenge Replikationen


```{r}
system2("openssl", "version", stdout = TRUE)

sessionInfo()

```


# Literaturverzeichnis
